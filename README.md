# Iterate: AI-Powered Data Cleaning & Quality Platform

> **Accelerating data cleaning processes with AI-generated code executionâ€”tailored to any dataset with high accuracy**

[![FastAPI](https://img.shields.io/badge/FastAPI-0.100+-009688?style=flat&logo=fastapi)](https://fastapi.tiangolo.com/)
[![React](https://img.shields.io/badge/React-18+-61DAFB?style=flat&logo=react&logoColor=black)](https://reactjs.org/)
[![TypeScript](https://img.shields.io/badge/TypeScript-5+-3178C6?style=flat&logo=typescript&logoColor=white)](https://www.typescriptlang.org/)
[![Anthropic Claude](https://img.shields.io/badge/Claude-4.5_Haiku-8B5CF6?style=flat)](https://www.anthropic.com/)
[![Python](https://img.shields.io/badge/Python-3.11+-3776AB?style=flat&logo=python&logoColor=white)](https://www.python.org/)

---

## ğŸ¯ The Problem: Data Cleaning is a Universal Bottleneck

Data professionals across industries spend **60-80% of their time** on data cleaning and preparationâ€”not on analysis or insights. This affects:

- **Data Scientists & Analysts**: Cleaning datasets before building models or dashboards
- **Operations Teams**: Preparing clean data for reporting and process optimization
- **Marketing & Sales**: Ensuring CRM data quality for campaigns and forecasting
- **Finance Analysts**: Validating financial data for accurate reporting
- **Researchers**: Standardizing datasets for reproducible analysis
- **Business Intelligence Teams**: Maintaining data warehouse integrity

Traditional approaches fall short:
- âŒ **Manual cleaning**: Time-consuming, error-prone, not scalable
- âŒ **Rule-based tools**: Inflexible, require constant maintenance for new datasets
- âŒ **LLM-based analysis**: Token limits prevent processing large datasets; prone to hallucination
- âŒ **Generic solutions**: Don't understand business context or domain-specific patterns

**The Cost**: Organizations lose millions in productivity, delayed insights, and poor data-driven decisions.

---

## ğŸ’¡ Our Solution: Intelligent, Adaptive Data Cleaning

Iterate leverages **AI-generated code execution** to provide automated, context-aware data quality analysis that adapts to any dataset structure. Our platform:

âœ… **Analyzes datasets of any size** (millions of rows) without token limits  
âœ… **Generates deterministic, verifiable results** by executing code locally  
âœ… **Understands business context** through intelligent dataset comprehension  
âœ… **Provides guided remediation** with conversational AI assistance  
âœ… **Scales instantly** to new dataset types without manual configuration  

### How It Works

Unlike traditional approaches, **we use LLMs as code generators, not data processors**:

```
Traditional Approach                    Iterate's Approach
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Send full dataset to LLM    â†’           Send 10-20 sample rows to LLM
     â†“                                       â†“
LLM processes data          â†’           LLM generates Python script
(200k token limit)                      (no size limits)
     â†“                                       â†“
Returns analysis            â†’           Script executes locally on full dataset
(may hallucinate)                       (deterministic results)
     â†“                                       â†“
âŒ Fails on large datasets              âœ… Processes millions of rows accurately
```

**Key Innovation**: AI generates intelligent validation scripts from small samples, then executes them locally on complete datasetsâ€”combining AI reasoning with code reliability.

---

## ğŸ—ï¸ Technical Architecture

### Multi-Agent Code Generation System

Our platform employs **specialized AI agents** that generate executable Python code instead of processing data directly:

#### 1. **Dataset Understanding Agent**
- **Input**: Column schema + 10-20 sample rows
- **Process**: LLM analyzes structure and infers business context
- **Output**: Business-focused dataset summary (no code generation)

#### 2. **Error Analysis Code Generator**
- **Input**: Dataset metadata, column types, sample rows
- **Process**: LLM generates comprehensive validation script with functions like:
  - `check_missing_values()` - Identifies null patterns
  - `check_duplicates()` - Detects duplicate records
  - `check_value_ranges()` - Validates numeric/date ranges
  - `check_category_drifts()` - Finds inconsistent categorical values
  - `check_id_consistency()` - Verifies identifier integrity
  - Custom validation functions based on dataset context
- **Output**: `detect_errors.py` (executable Python script)

#### 3. **Local Script Execution Engine**
- Runs generated validation scripts on **full dataset** (all rows)
- No token limitsâ€”processes datasets with millions of rows
- Captures structured error reports with evidence
- Deterministic results (same input = same output)

#### 4. **Error Correction Code Generator**
- **Input**: Error report + conversation context
- **Process**: LLM generates targeted remediation script
- **Output**: `fix_errors.py` (executable correction script)

#### 5. **Smart Fix Follow-up Agent**
- Generates contextual questions for ambiguous data issues
- Guides users through correction decisions
- Adapts generated code based on user responses

### Architecture Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Dataset Upload                            â”‚
â”‚         (CSV/Excel â†’ Pandas â†’ Storage + Sampling)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          ğŸ¤– Dataset Understanding Agent                      â”‚
â”‚  Analyzes: Schema + 10-20 sample rows                       â”‚
â”‚  Outputs: Business context summary                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          ğŸ¤– Error Analysis Code Generator                    â”‚
â”‚  Generates: Complete Python validation script               â”‚
â”‚  Based on: Dataset structure + domain patterns              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              âš™ï¸ Local Script Execution                      â”‚
â”‚  Executes: Generated script on FULL dataset                 â”‚
â”‚  Processes: Millions of rows without limits                 â”‚
â”‚  Returns: Structured error report with evidence             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          ğŸ¤– Error Correction Generator                       â”‚
â”‚  Generates: Targeted remediation scripts                    â”‚
â”‚  Adapts: Based on user guidance via Smart Fix Agent         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why This Architecture Matters

**Solving LLM Limitations**:
- **Token Limits**: LLMs can't process large datasets directly (200k max tokens â‰ˆ 50k rows)
- **Hallucination**: Direct LLM analysis can produce unreliable results
- **Cost**: Processing millions of rows through LLM APIs is prohibitively expensive

**Our Code-Generation Approach**:
- **Unlimited Scale**: Generated scripts process any dataset size
- **Deterministic**: Code execution produces verifiable, consistent results
- **Cost-Efficient**: One API call generates reusable, executable scripts
- **Transparent**: Users can inspect and modify generated validation logic

---

## ğŸ› ï¸ Technology Stack

### Backend
- **Framework**: FastAPI (async Python web framework)
- **AI/ML**: 
  - LangChain + Anthropic Claude 4.5 Haiku (code generation)
  - Sentence Transformers (embeddings for context retrieval)
- **Data Processing**: Pandas, OpenPyXL (CSV/Excel handling)
- **Database**: 
  - MongoDB (conversation history, dataset metadata)
  - PostgreSQL + pgvector (vector embeddings)
- **Execution**: subprocess (sandboxed Python script execution)

### Frontend
- **Framework**: React 18 + TypeScript
- **Build Tool**: Vite
- **UI Components**: Radix UI + shadcn/ui (accessible component library)
- **Styling**: Tailwind CSS
- **State Management**: TanStack Query (data fetching/caching)
- **Routing**: React Router

### Infrastructure
- **Package Management**: Bun (frontend), pip (backend)
- **Development**: Hot-reload dev servers, TypeScript strict mode
- **API Communication**: RESTful endpoints + Server-Sent Events (streaming)

---

## ğŸš€ Key Features

### 1. **Intelligent Dataset Understanding**
- Automatic schema detection and type inference
- Business context extraction from column names and values
- Smart sampling for representative data analysis

### 2. **Comprehensive Error Detection**
- Missing value analysis with pattern recognition
- Duplicate detection (exact and fuzzy matching)
- Type inconsistency identification
- Range validation (dates, numerics, categories)
- Cross-column relationship checks
- Temporal pattern analysis

### 3. **Guided Data Remediation**
- **Quick Fixes**: One-click corrections for common issues
- **Smart Fixes**: AI-guided interactive workflows for complex scenarios
- Conversational clarification for ambiguous data problems
- Preview changes before applying

### 4. **Context-Aware Chat Interface**
- Natural language queries about your dataset
- Dataset-specific Q&A powered by MongoDB memory
- Embedding-based context retrieval for relevant responses

### 5. **Production-Ready Design**
- Async processing for large datasets
- Streaming progress updates via SSE
- Error handling and fallback mechanisms
- Comprehensive logging and observability

---

## ğŸ“Š Real-World Impact

### Use Cases

**E-commerce Operations**
- Clean product catalogs with inconsistent categories
- Standardize supplier names and SKU formats
- Validate pricing and inventory data

**Marketing Analytics**
- Deduplicate CRM contacts across systems
- Standardize campaign tracking parameters
- Fix date formatting in event logs

**Financial Reporting**
- Validate transaction data for completeness
- Detect anomalies in expense reports
- Ensure regulatory compliance in audit trails

**Research & Academia**
- Standardize survey response formats
- Clean experimental data before analysis
- Merge datasets from multiple sources

**Sales Operations**
- Clean opportunity data in CRM systems
- Validate lead scoring attributes
- Fix contact information formatting

---

## ğŸ”§ Getting Started

### Prerequisites
- **Backend**: Python 3.11+, pip
- **Frontend**: Node.js 18+, Bun
- **Services**: MongoDB, PostgreSQL (with pgvector)
- **API Keys**: Anthropic API key for Claude

### Installation

1. **Clone the repository**
```bash
git clone <repository-url>
cd iterate-hack
```

2. **Backend Setup**
```bash
cd Iterate-Hackathon-Backend

# Install dependencies
pip install -r requirements.txt

# Configure environment
cp .env.example .env
# Edit .env with your API keys and database URLs

# Run migrations (if applicable)
# Start the server
uvicorn app.main:app --reload --port 8000
```

3. **Frontend Setup**
```bash
cd frontend

# Install dependencies
bun install

# Start development server
bun run dev
```

4. **Access the Application**
- Frontend: http://localhost:5173
- Backend API: http://localhost:8000
- API Docs: http://localhost:8000/docs

---

## ğŸ“ Project Structure

```
iterate-hack/
â”œâ”€â”€ Iterate-Hackathon-Backend/      # FastAPI backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ agent.py               # Agent orchestration & LLM config
â”‚   â”‚   â”œâ”€â”€ tools.py               # Code generation agents
â”‚   â”‚   â”œâ”€â”€ main.py                # API endpoints
â”‚   â”‚   â”œâ”€â”€ chat.py                # Conversational interface
â”‚   â”‚   â”œâ”€â”€ dataset_store.py       # Dataset persistence
â”‚   â”‚   â”œâ”€â”€ excel_context.py       # Excel/CSV context building
â”‚   â”‚   â””â”€â”€ config.py              # Settings management
â”‚   â”œâ”€â”€ data/                      # Uploaded datasets (gitignored)
â”‚   â”œâ”€â”€ scripts/                   # Generated validation scripts
â”‚   â”œâ”€â”€ docs/                      # Architecture documentation
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ frontend/                       # React TypeScript frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/            # UI components (shadcn/ui)
â”‚   â”‚   â”œâ”€â”€ pages/                 # Route components
â”‚   â”‚   â”œâ”€â”€ context/               # React context providers
â”‚   â”‚   â”œâ”€â”€ hooks/                 # Custom React hooks
â”‚   â”‚   â”œâ”€â”€ lib/                   # Utilities
â”‚   â”‚   â””â”€â”€ types/                 # TypeScript definitions
â”‚   â””â”€â”€ package.json
â”‚
â””â”€â”€ docs/                          # Project documentation
    â”œâ”€â”€ agent-contracts.md         # Agent I/O specifications
    â”œâ”€â”€ ai-agent-integration-plan.md
    â””â”€â”€ code-execution-agent-requirements.md
```

---

## ğŸ“ Documentation

- **[Agent Architecture](Iterate-Hackathon-Backend/docs/AGENT_ARCHITECTURE.md)**: Deep dive into code-generation system
- **[Agent Contracts](docs/agent-contracts.md)**: API schemas and data structures
- **[Integration Plan](docs/ai-agent-integration-plan.md)**: Implementation roadmap
- **[Backend README](Iterate-Hackathon-Backend/README.md)**: Detailed backend documentation

---

## ğŸ§ª Development

### Running Tests
```bash
# Backend tests
cd Iterate-Hackathon-Backend
pytest

# Frontend tests (if configured)
cd frontend
bun test
```

### Code Quality
```bash
# Backend linting
ruff check app/

# Frontend linting
cd frontend
bun run lint
```

---

## ğŸŒŸ What Makes Iterate Different

| Traditional Tools | LLM-Direct Analysis | **Iterate (Code Generation)** |
|-------------------|---------------------|-------------------------------|
| Manual rules      | Sends data to LLM   | **LLM generates validation code** |
| Inflexible        | 200k token limit    | **Unlimited dataset size** |
| No context        | May hallucinate     | **Deterministic execution** |
| Slow updates      | High API costs      | **Cost-efficient (one API call)** |
| Generic           | Black box           | **Transparent, inspectable scripts** |

**Iterate combines the best of both worlds**: AI reasoning for context understanding + code execution for reliability and scale.

---

## ğŸ›£ï¸ Roadmap

- [x] Core code-generation agent architecture
- [x] Dataset understanding & error detection
- [x] Smart fix workflows with conversational guidance
- [x] Streaming progress updates
- [ ] Multi-dataset comparison and merging
- [ ] Custom validation rule templates
- [ ] Scheduled data quality monitoring
- [ ] Team collaboration features
- [ ] API for programmatic access
- [ ] Cloud deployment (Azure/AWS)

---

## ğŸ‘¥ Team

Built during the Iterate Hackathon by a team passionate about solving real-world data quality challenges.

---

## ğŸ“„ License

[Add your license here]

---

## ğŸ¤ Contributing

We welcome contributions! Please see our contributing guidelines for details on how to:
- Report bugs
- Suggest features
- Submit pull requests

---

## ğŸ“ Support

For questions, issues, or feedback:
- Open an issue on GitHub
- [Contact information]

---

**Iterate**: *Because clean data shouldn't be this hard.*
